import bs4
from urllib.request import urlopen as uReq
from bs4 import BeautifulSoup as soup
import numpy as np
import pandas as pd
import time


my_url = 'http://www.espn.com/nba/statistics/player/_/stat/scoring-per-game/sort/avgPoints/year/2018'

uClient = uReq(my_url)
# calling function url open inside of a module called
# request inside of a package called urllib
# Calling my url essentially

page_html = uClient.read()
uClient.close()
page_soup = soup(page_html, "html.parser")
title_PTS = "PTS"
title_AST = "AST"
title_3PM = "3PM"

table  = page_soup.find('table', {'class':'tablehead'})

rows = table.find_all('tr')
columns = table.find_all('td')
player = table.find_all(text = True )

array = player

# class Playyer:
#     def __init__(self, rank, name, gp, mpg, pts, fgm, fg_percent, three, three_perc, ftm, ft_percent):
#
#         self.rank = rank
#         self.name = name
#         self.gp = gp
#         self.mpg = mpg
#         self.pts = pts
#         self.fgm = fgm
#         self.fg_percent = fg_percent
#         self.three = three
#         self. three_perc = three_perc
#         self.ftm =ftm
#         self.ft_percent = ft_percent
#
#
#
#     def __str__(self):
#         return str((self.rank, self.name, self.gp,
#                      self.mpg,self.pts,self.fgm,self.fg_percent,
#
#
#
#
#
#
#
#
#

# Names:


i=1


players_1 = rows[i].text[1:]

while i<10:
    i+=1
    string = rows[i].text[1:]
    split_1 = string.split(',')



    if rows[i].text[1:] != str('PLAYERTEAMG'):
        names_1 = split_1[0]


# for next 9 players
j=10
while j<40:
    j +=1
    string_2 = rows[j].text[2:]
    split_2 = string_2.split(',')

    if rows[j].text[2:] != str('PLAYERTEAMGPMPGPTSFGM-FGAFG%3PM-3PA3P%FTM-FTAFT%'):
            names_2 = split_2[0]
            rest_1 = split_2[1]
            PTS_1 = rest_1[8:12]
            AST_1 = rest_1[13:16]
            RBS_1 = rest_1[17-22]
            Thper_1 = rest_1[23-27]

            Nba_Data = {
                'Player' : str(names_2),
                'PTS' : PTS_1,
                'AST' : AST_1,
                'RBS_1': RBS_1,
                '3%' : Thper_1

            }
            i=0
            # while i< 10:
            #     i+=1
            #     rank=i
            df = pd.DataFrame(Nba_Data, columns=['Player', 'PTS', 'AST','RBS_1','3%'], index=[0])

            writer = pd.ExcelWriter(time.strftime("NBA DATA"))
            df.to_excel(writer, 'Sheet1')
            writer.save()







k=1
while k<10:
    k+=1
    string11 = rows[k].text[1:]
    split_11 = string.split('82')






# columns =[v.text.replace('\n', '') for v in rows[0].find_all('th')]
#
# for tr in page_soupsoup.find_all('tr')[2:]:
#     tds = tr.find_all('td')
#     print "Nome: %s, Cognome: %s, Email: %s" % \
#           (tds[0].text, tds[1].text, tds[2].text
#

















# import bs4
# from urllib.request import urlopen as uReq
# from bs4 import BeautifulSoup as soup
#
# my_url = 'https://www.newegg.ca/Product/ProductList.aspx?Submit=ENE&DEPA=0&Order=BESTMATCH&Description=graphics+card+&N=-1&isNodeId=10'
#
# uClient = uReq(my_url)
# # calling function url open inside of a module called
# # request inside of a package called urllib
#
# page_html = uClient.read()
# uClient.close()
# page_soup = soup(page_html, "html.parser")
#
# # grabs each container
# containers = page_soup.findAll("div", {"class":"item-container"})
#
# container = containers[0]
#
# print(container.div.div.a)
#
# C:\Users\sverc\.PyCharmCE2018.2\config\scratches\Web_scratch .py
